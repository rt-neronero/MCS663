{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa371eb",
   "metadata": {},
   "source": [
    "# Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc5235",
   "metadata": {},
   "source": [
    "### A. Identify a suitable dataset relevant to imaging techniques, specifically for classification. Ensure that the dataset contains at least 1,000 images and includes a mix of good and poor-quality images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1e5e6",
   "metadata": {},
   "source": [
    "> Dataset Link: https://universe.roboflow.com/tugas-akhir-icad/proyek-akhir-icad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3697355",
   "metadata": {},
   "source": [
    "### B. List the names of object identification techniques. Try to find the latest techniques from recent research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bf230",
   "metadata": {},
   "source": [
    "> RT-DETR or Real-Time Detection Transformer is an object detection model developed by Peking University and Baidu said to be capable of being equal or outperform even YOLO models when it comes to object detection. It integrates a transformer-based architecture which helps it achieve high accuracy while maintaining real-time performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca5819",
   "metadata": {},
   "source": [
    "### C. Apply any two object identification techniques to the dataset. Analyze which technique performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e51452",
   "metadata": {},
   "source": [
    "> YOLOv12 (yolo.ipynb) and RT-DETR(rt-detr.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e7fd",
   "metadata": {},
   "source": [
    "### D. Is there an automated way to calculate how many objects were correctly identified and in how many objects the bounding boxes were misplaced?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6a6c7",
   "metadata": {},
   "source": [
    "> Yes, there is an automated way to calculate how many objects were correctly identified and how many had misplaced bounding boxes, typically through object detection evaluation metrics. These are computed using tools and libraries that support evaluation against ground truth annotations, such as **Mean Average Precision (mAP)**, **Intersection over Union (IoU)**, and **confusion matrices**. If the IoU between a predicted bounding box and the corresponding ground truth box exceeds a set threshold (commonly 0.5), it is considered a correct detection. Misplaced bounding boxes are those with low IoU or those matched to the wrong class. In YOLO workflows, tools such as **ultralytics's evaluation functions**, or external benchmarking libraries like **COCO API** or **pycocotools**, can automate this process after training or during validation. These tools not only count true positives (correctly identified objects) but also highlight false positives and localization errors, helping assess both classification and localization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39482cc",
   "metadata": {},
   "source": [
    "### E. Is it possible to automatically adjust the size of the bounding box for each image? For example, if the dataset contains images of varying sizes, how does using a fixed-size bounding box affect the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b2c25",
   "metadata": {},
   "source": [
    "> As far as we know, bounding boxes can be automatically adjusted based on image sizes. This can be done through the normalization coordinates ranging from a converted value of 0 to 1 based on image dimensions and then scaling the bounding boxes based on those. Though if a dataset containing various sizes of images were to use a fixed-size bounding box, pretty sure it is likely for the accuracy and prediction results to be largely affected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
